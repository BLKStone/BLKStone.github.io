<!doctype html>
<html class="theme-next use-motion theme-next-mist">
<head>
  

<meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>






  <link rel="stylesheet" type="text/css" href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5"/>


<link rel="stylesheet" type="text/css" href="/css/main.css?v=0.4.3"/>


    <meta name="description" content="关于计算机科学的学习经历与精彩文章分享。" />



  <meta name="keywords" content="神经网络,python," />





  <link rel="shorticon icon" type="image/x-icon" href="/favicon.ico?v=0.4.3" />



<script type="text/javascript" id="hexo.configuration">
  var CONFIG = {
    scheme: 'Mist',
    analytics: {
      google: ''
    },
    sidebar: 'post'
  };
</script>


  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?351a523b2429a0dda1b90bb708a00fad";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>



  <title> 使用python从头开始实现一个神经网络 // Neurohazard </title>
</head>

<body>
  <!--[if lte IE 8]>
  <div style=' clear: both; height: 59px; padding:0 0 0 15px; position: relative;margin:0 auto;'>
    <a href="http://windows.microsoft.com/en-US/internet-explorer/products/ie/home?ocid=ie6_countdown_bannercode">
      <img src="http://7u2nvr.com1.z0.glb.clouddn.com/picouterie.jpg" border="0" height="42" width="820"
           alt="You are using an outdated browser. For a faster, safer browsing experience, upgrade for free today or use other browser ,like chrome firefox safari."
           style='margin-left:auto;margin-right:auto;display: block;'/>
    </a>
  </div>
<![endif]-->

  <div class="container one-column page-post-detail">
    <div class="headband"></div>

    <div id="header" class="header">
      <div class="header-inner">
        <h1 class="site-meta">
  <span class="logo-line-before"><i></i></span>
  <a href="/" class="brand">
      <span class="logo">
        <i class="icon-logo"></i>
      </span>
      <span class="site-title">Neurohazard</span>
  </a>
  <span class="logo-line-after"><i></i></span>
</h1>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<div class="site-nav">
  

  
    <ul id="menu" class="menu ">
      
        
        <li class="menu-item menu-item-home">
          <a href="/">
            <i class="menu-item-icon icon-home"></i> <br />
            首頁
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about">
            <i class="menu-item-icon icon-about"></i> <br />
            關於
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives">
            <i class="menu-item-icon icon-archives"></i> <br />
            歸檔
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags">
            <i class="menu-item-icon icon-tags"></i> <br />
            標籤
          </a>
        </li>
      
        
        <li class="menu-item menu-item-friend">
          <a href="/friend">
            <i class="menu-item-icon icon-friend"></i> <br />
            友情鏈接
          </a>
        </li>
      
        
        <li class="menu-item menu-item-reading-list">
          <a href="/reading-list">
            <i class="menu-item-icon icon-reading-list"></i> <br />
            閱讀清單
          </a>
        </li>
      
    </ul>
  

  
</div>


      </div>
    </div>

    <div id="main" class="main">
      <div class="main-inner">
        <div id="content" class="content">
          

  <div id="posts" class="posts-expand">
    

  <div class="post post-type-normal ">
    <div class="post-header">

      
      
        <h1 class="post-title">
          
          
            
              使用python从头开始实现一个神经网络
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          發表於 2015-09-27
        </span>

        
          <span class="post-category">
            &nbsp; | &nbsp; 分類於
            
              <a href="/categories/机器学习/">机器学习</a>

              
              

            
          </span>
        

        
          
            <span class="post-comments-count">
            &nbsp; | &nbsp;
            <a href="/2015/09/27/nn-from-scratch/#comments" >
              <span class="post-comments-count ds-thread-count" data-thread-key="2015/09/27/nn-from-scratch/"></span>
            </a>
          </span>
          
        
      </div>
    </div>

    <div class="post-body">

      
      

      
        <h1 id="导入相关包"><a href="#导入相关包" class="headerlink" title="导入相关包"></a>导入相关包</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Package imports</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> arange</span><br><span class="line"><span class="keyword">import</span> sklearn</span><br><span class="line"><span class="keyword">import</span> sklearn.datasets</span><br><span class="line"><span class="keyword">import</span> sklearn.linear_model</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"></span><br><span class="line"><span class="comment"># Display plots inline and change default figure size</span></span><br><span class="line">matplotlib.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">10.0</span>, <span class="number">8.0</span>)</span><br></pre></td></tr></table></figure>
<h1 id="生成数据集"><a href="#生成数据集" class="headerlink" title="生成数据集"></a>生成数据集</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Generate a dataset and plot it</span></span><br><span class="line"><span class="comment"># 生成两个交错的半圆的数据集</span></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line">X, y = sklearn.datasets.make_moons(<span class="number">200</span>, noise=<span class="number">0.20</span>)</span><br><span class="line">plt.scatter(X[:,<span class="number">0</span>], X[:,<span class="number">1</span>], s=<span class="number">40</span>, c=y, cmap=plt.cm.Spectral)</span><br></pre></td></tr></table></figure>
<h1 id="绘制决策边界"><a href="#绘制决策边界" class="headerlink" title="绘制决策边界"></a>绘制决策边界</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Helper function to plot a decision boundary.</span></span><br><span class="line"><span class="comment"># If you don't fully understand this function don't worry, it just generates the contour plot below.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_decision_boundary</span><span class="params">(pred_func)</span>:</span></span><br><span class="line">    <span class="comment"># Set min and max values and give it some padding</span></span><br><span class="line">    x_min, x_max = X[:, <span class="number">0</span>].min() - <span class="number">.5</span>, X[:, <span class="number">0</span>].max() + <span class="number">.5</span></span><br><span class="line">    y_min, y_max = X[:, <span class="number">1</span>].min() - <span class="number">.5</span>, X[:, <span class="number">1</span>].max() + <span class="number">.5</span></span><br><span class="line">    h = <span class="number">0.01</span></span><br><span class="line">    <span class="comment"># Generate a grid of points with distance h between them</span></span><br><span class="line">    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))</span><br><span class="line">    <span class="comment"># Predict the function value for the whole gid</span></span><br><span class="line">    Z = pred_func(np.c_[xx.ravel(), yy.ravel()])</span><br><span class="line">    Z = Z.reshape(xx.shape)</span><br><span class="line">    <span class="comment"># Plot the contour and training examples</span></span><br><span class="line">    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)</span><br><span class="line">    plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y, cmap=plt.cm.Spectral)</span><br></pre></td></tr></table></figure>
<h1 id="logistic-rgeression-classifier-无法解决问题"><a href="#logistic-rgeression-classifier-无法解决问题" class="headerlink" title="logistic rgeression classifier 无法解决问题"></a>logistic rgeression classifier 无法解决问题</h1><p>模型训练<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Train the logistic rgeression classifier</span></span><br><span class="line">clf = sklearn.linear_model.LogisticRegressionCV()</span><br><span class="line">clf.fit(X, y)</span><br></pre></td></tr></table></figure></p>
<p>查看结果<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Plot the decision boundary</span></span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: clf.predict(x))</span><br><span class="line">plt.title(<span class="string">"Logistic Regression"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<h1 id="神经网络的初始化参数"><a href="#神经网络的初始化参数" class="headerlink" title="神经网络的初始化参数"></a>神经网络的初始化参数</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">num_examples = len(X) <span class="comment"># training set size</span></span><br><span class="line">nn_input_dim = <span class="number">2</span> <span class="comment"># input layer dimensionality</span></span><br><span class="line">nn_output_dim = <span class="number">2</span> <span class="comment"># output layer dimensionality</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Gradient descent parameters (I picked these by hand)</span></span><br><span class="line">epsilon = <span class="number">0.01</span> <span class="comment"># learning rate for gradient descent</span></span><br><span class="line">reg_lambda = <span class="number">0.01</span> <span class="comment"># regularization strength</span></span><br></pre></td></tr></table></figure>
<h1 id="计算损失函数"><a href="#计算损失函数" class="headerlink" title="计算损失函数"></a>计算损失函数</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Helper function to evaluate the total loss on the dataset</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calculate_loss</span><span class="params">(model)</span>:</span></span><br><span class="line">    W1, b1, W2, b2 = model[<span class="string">'W1'</span>], model[<span class="string">'b1'</span>], model[<span class="string">'W2'</span>], model[<span class="string">'b2'</span>]</span><br><span class="line">    <span class="comment"># Forward propagation to calculate our predictions</span></span><br><span class="line">    z1 = X.dot(W1) + b1</span><br><span class="line">    a1 = np.tanh(z1)</span><br><span class="line">    z2 = a1.dot(W2) + b2</span><br><span class="line">    exp_scores = np.exp(z2)</span><br><span class="line">    probs = exp_scores / np.sum(exp_scores, axis=<span class="number">1</span>, keepdims=<span class="keyword">True</span>)</span><br><span class="line">    <span class="comment"># Calculating the loss</span></span><br><span class="line">    corect_logprobs = -np.log(probs[range(num_examples), y])</span><br><span class="line">    data_loss = np.sum(corect_logprobs)</span><br><span class="line">    <span class="comment"># Add regulatization term to loss (optional)</span></span><br><span class="line">    data_loss += reg_lambda/<span class="number">2</span> * (np.sum(np.square(W1)) + np.sum(np.square(W2)))</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.</span>/num_examples * data_loss</span><br></pre></td></tr></table></figure>
<h1 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Helper function to predict an output (0 or 1)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(model, x)</span>:</span></span><br><span class="line">    W1, b1, W2, b2 = model[<span class="string">'W1'</span>], model[<span class="string">'b1'</span>], model[<span class="string">'W2'</span>], model[<span class="string">'b2'</span>]</span><br><span class="line">    <span class="comment"># Forward propagation</span></span><br><span class="line">    z1 = x.dot(W1) + b1</span><br><span class="line">    a1 = np.tanh(z1)</span><br><span class="line">    z2 = a1.dot(W2) + b2</span><br><span class="line">    exp_scores = np.exp(z2)</span><br><span class="line">    probs = exp_scores / np.sum(exp_scores, axis=<span class="number">1</span>, keepdims=<span class="keyword">True</span>)</span><br><span class="line">    <span class="keyword">return</span> np.argmax(probs, axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h1 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># This function learns parameters for the neural network and returns the model.</span></span><br><span class="line"><span class="comment"># - nn_hdim: Number of nodes in the hidden layer</span></span><br><span class="line"><span class="comment"># - num_passes: Number of passes through the training data for gradient descent</span></span><br><span class="line"><span class="comment"># - print_loss: If True, print the loss every 1000 iterations</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_model</span><span class="params">(nn_hdim, num_passes=<span class="number">20000</span>, print_loss=False)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize the parameters to random values. We need to learn these.</span></span><br><span class="line">    np.random.seed(<span class="number">0</span>)</span><br><span class="line">    W1 = np.random.randn(nn_input_dim, nn_hdim) / np.sqrt(nn_input_dim)</span><br><span class="line">    b1 = np.zeros((<span class="number">1</span>, nn_hdim))</span><br><span class="line">    W2 = np.random.randn(nn_hdim, nn_output_dim) / np.sqrt(nn_hdim)</span><br><span class="line">    b2 = np.zeros((<span class="number">1</span>, nn_output_dim))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># This is what we return at the end</span></span><br><span class="line">    model = &#123;&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Gradient descent. For each batch...</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> xrange(<span class="number">0</span>, num_passes):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Forward propagation</span></span><br><span class="line">        z1 = X.dot(W1) + b1</span><br><span class="line">        a1 = np.tanh(z1)</span><br><span class="line">        z2 = a1.dot(W2) + b2</span><br><span class="line">        exp_scores = np.exp(z2)</span><br><span class="line">        probs = exp_scores / np.sum(exp_scores, axis=<span class="number">1</span>, keepdims=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Backpropagation</span></span><br><span class="line">        delta3 = probs</span><br><span class="line">        delta3[range(num_examples), y] -= <span class="number">1</span></span><br><span class="line">        dW2 = (a1.T).dot(delta3)</span><br><span class="line">        db2 = np.sum(delta3, axis=<span class="number">0</span>, keepdims=<span class="keyword">True</span>)</span><br><span class="line">        delta2 = delta3.dot(W2.T) * (<span class="number">1</span> - np.power(a1, <span class="number">2</span>))</span><br><span class="line">        dW1 = np.dot(X.T, delta2)</span><br><span class="line">        db1 = np.sum(delta2, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Add regularization terms (b1 and b2 don't have regularization terms)</span></span><br><span class="line">        dW2 += reg_lambda * W2</span><br><span class="line">        dW1 += reg_lambda * W1</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Gradient descent parameter update</span></span><br><span class="line">        W1 += -epsilon * dW1</span><br><span class="line">        b1 += -epsilon * db1</span><br><span class="line">        W2 += -epsilon * dW2</span><br><span class="line">        b2 += -epsilon * db2</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Assign new parameters to the model</span></span><br><span class="line">        model = &#123; <span class="string">'W1'</span>: W1, <span class="string">'b1'</span>: b1, <span class="string">'W2'</span>: W2, <span class="string">'b2'</span>: b2&#125;</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Optionally print the loss.</span></span><br><span class="line">        <span class="comment"># This is expensive because it uses the whole dataset, so we don't want to do it too often.</span></span><br><span class="line">        <span class="keyword">if</span> print_loss <span class="keyword">and</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">          <span class="keyword">print</span> <span class="string">"Loss after iteration %i: %f"</span> %(i, calculate_loss(model))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>
<h1 id="模型实例"><a href="#模型实例" class="headerlink" title="模型实例"></a>模型实例</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Build a model with a 3-dimensional hidden layer</span></span><br><span class="line">model = build_model(<span class="number">3</span>, print_loss=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot the decision boundary</span></span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: predict(model, x))</span><br><span class="line">plt.title(<span class="string">"Decision Boundary for hidden layer size 3"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h1 id="不同的隐含层规模"><a href="#不同的隐含层规模" class="headerlink" title="不同的隐含层规模"></a>不同的隐含层规模</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">16</span>, <span class="number">32</span>))</span><br><span class="line">hidden_layer_dimensions = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">20</span>, <span class="number">50</span>]</span><br><span class="line"><span class="keyword">for</span> i, nn_hdim <span class="keyword">in</span> enumerate(hidden_layer_dimensions):</span><br><span class="line">    plt.subplot(<span class="number">5</span>, <span class="number">2</span>, i+<span class="number">1</span>)</span><br><span class="line">    plt.title(<span class="string">'Hidden Layer size %d'</span> % nn_hdim)</span><br><span class="line">    model = build_model(nn_hdim)</span><br><span class="line">    plot_decision_boundary(<span class="keyword">lambda</span> x: predict(model, x))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h1 id="其他值得关注的点"><a href="#其他值得关注的点" class="headerlink" title="其他值得关注的点"></a>其他值得关注的点</h1><h2 id="查看tanh函数"><a href="#查看tanh函数" class="headerlink" title="查看tanh函数"></a>查看tanh函数</h2><figure class="highlight cos"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># http:<span class="comment">//stackoverflow.com/questions/7267226/range-for-floats</span></span><br><span class="line"># 查看tanh</span><br><span class="line"><span class="keyword">X</span>=arange(-<span class="number">10</span>,<span class="number">10</span>,<span class="number">0.05</span>)</span><br><span class="line"><span class="keyword">print</span> <span class="keyword">X</span>.shape</span><br><span class="line">y=np.tanh(<span class="keyword">X</span>)</span><br><span class="line">plt.scatter(<span class="keyword">X</span>, y, c='<span class="keyword">r</span>')</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="softmax函数"><a href="#softmax函数" class="headerlink" title="softmax函数"></a>softmax函数</h2><p><a href="http://blog.csdn.net/kevinew/article/details/9407367" target="_blank" rel="external">http://blog.csdn.net/kevinew/article/details/9407367</a></p>
<h2 id="关于隐含层的规模"><a href="#关于隐含层的规模" class="headerlink" title="关于隐含层的规模"></a>关于隐含层的规模</h2><blockquote>
<p>But higher dimensionality comes at a cost. First, more computation is required to make predictions and learn the network parameters.A bigger number of parameters also means we become more prone to overfitting our data.</p>
</blockquote>
<p>过大的隐含层规模有两个缺点，首先是建完的模型有更多的参数需要运算;另外隐含层过大非常容易导致数据过拟合。</p>
<p>关于合适的隐含层规模(hidden layer size)，《Matlab在数学建模中的应用》有一小节简单做了讨论，提供了两个经验公式,当然这里的结论只是仅供参考而已。</p>
<p>$$ l=\sqrt{m+n}+a $$</p>
<p>$$ l=\sqrt{0.43mn+0.12n^{2}+2.54m+0.77n+0.35+0.51} $$</p>
<p>其中m,n分别为输入层规模与输出层规模，a为1～10之间的常数,l为需要的隐含层规模。</p>
<h2 id="关于反向传播的细节"><a href="#关于反向传播的细节" class="headerlink" title="关于反向传播的细节"></a>关于反向传播的细节</h2><blockquote>
<p>I won’t go into detail how backpropagation works, but there are many excellent explanations (here or here) floating around the web.</p>
</blockquote>
<p><a href="http://colah.github.io/posts/2015-08-Backprop/" target="_blank" rel="external">http://colah.github.io/posts/2015-08-Backprop/</a></p>
<p><a href="http://cs231n.github.io/optimization-2/" target="_blank" rel="external">http://cs231n.github.io/optimization-2/</a></p>
<h2 id="变学习速率"><a href="#变学习速率" class="headerlink" title="变学习速率"></a>变学习速率</h2><blockquote>
<p>So if you are serious you’ll want to use one of these, and ideally you would also decay the learning rate over time.</p>
</blockquote>
<p><a href="http://cs231n.github.io/neural-networks-3/#anneal" target="_blank" rel="external">http://cs231n.github.io/neural-networks-3/#anneal</a></p>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p>[1] <a href="http://python.jobbole.com/82208/" target="_blank" rel="external">从头开始实现神经网络：入门</a><br>[2] <a href="http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/" target="_blank" rel="external">Implementing a Neural Network from Scratch in Python – An Introduction</a><br>[3] <a href="https://github.com/dennybritz/nn-from-scratch" target="_blank" rel="external">nn-from-scratch (github)</a><br>[4] <a href="http://scikit-learn.org/dev/modules/neural_networks_supervised.html" target="_blank" rel="external">sklearn的多层感知器</a></p>

      
    </div>

    <div class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/神经网络/"> #神经网络 </a>
          
            <a href="/tags/python/"> #python </a>
          
        </div>
      

      
        <div class="post-nav">
          <div class="post-nav-prev post-nav-item">
            
              <a href="/2015/10/02/python-with-as/">理解Python中的with…as…语法</a>
            
          </div>

          <div class="post-nav-next post-nav-item">
            
              <a href="/2015/09/26/python-sort/">各种常见排序算法的Python实现</a>
            
          </div>
        </div>
      

      
      
    </div>
  </div>



    
      <div class="post-spread">
        <!-- JiaThis Button BEGIN -->
<div class="jiathis_style">
  <a class="jiathis_button_tsina"></a>
  <a class="jiathis_button_tqq"></a>
  <a class="jiathis_button_weixin"></a>
  <a class="jiathis_button_cqq"></a>
  <a class="jiathis_button_douban"></a>
  <a class="jiathis_button_renren"></a>
  <a class="jiathis_button_qzone"></a>
  <a class="jiathis_button_kaixin001"></a>
  <a class="jiathis_button_copy"></a>
  <a href="http://www.jiathis.com/share" class="jiathis jiathis_txt jiathis_separator jtico jtico_jiathis" target="_blank"></a>
  <a class="jiathis_counter_style"></a>
</div>
<script type="text/javascript" >
  var jiathis_config={
    hideMore:false
  }
</script>
<script type="text/javascript" src="http://v3.jiathis.com/code/jia.js" charset="utf-8"></script>
<!-- JiaThis Button END -->

      </div>
    

    
      <div class="comments" id="comments">
        
          <div class="ds-thread" data-thread-key="2015/09/27/nn-from-scratch/"
               data-title="使用python从头开始实现一个神经网络" data-url="http://blkstone.github.io/2015/09/27/nn-from-scratch/">
          </div>
        
      </div>
    
  </div>


        </div>

        
      </div>


      
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <div id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目錄
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            本站概覽
          </li>
        </ul>
      

      <div class="site-overview">
        <div class="site-author motion-element">
          <img class="site-author-image" src="/images/avatar.jpg" alt="Ray" />
          <p class="site-author-name">Ray</p>
        </div>
        <p class="site-description motion-element">关于计算机科学的学习经历与精彩文章分享。</p>
        <div class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">220</span>
              <span class="site-state-item-name">文章</span>
            </a>
          </div>

          <div class="site-state-item site-state-categories">
            
              <span class="site-state-item-count">13</span>
              <span class="site-state-item-name">分類</span>
              
          </div>

          <div class="site-state-item site-state-tags">
            <a href="/tags">
              <span class="site-state-item-count">75</span>
              <span class="site-state-item-name">標籤</span>
              </a>
          </div>

        </div>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
              <a href="https://github.com/BLKStone" target="_blank">GitHub</a>
            </span>
            
              <span class="links-of-author-item">
              <a href="http://blkstone.github.io/" target="_blank">Twitter</a>
            </span>
            
              <span class="links-of-author-item">
              <a href="http://blkstone.github.io/" target="_blank">Weibo</a>
            </span>
            
              <span class="links-of-author-item">
              <a href="http://blkstone.github.io/" target="_blank">DouBan</a>
            </span>
            
              <span class="links-of-author-item">
              <a href="http://blkstone.github.io/" target="_blank">ZhiHu</a>
            </span>
            
          
        </div>

        
        
          <div class="cc-license motion-element">
            <a href="http://creativecommons.org/licenses/by-nc-sa/4.0" class="cc-opacity" target="_blank">
              <img src="/images/cc-by-nc-sa.svg" alt="Creative Commons" />
            </a>
          </div>
        

      </div>

      
        <div class="post-toc-wrap sidebar-panel-active">
          <div class="post-toc-indicator-top post-toc-indicator"></div>
          <div class="post-toc">
            
            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#导入相关包"><span class="nav-number">1.</span> <span class="nav-text">导入相关包</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#生成数据集"><span class="nav-number">2.</span> <span class="nav-text">生成数据集</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#绘制决策边界"><span class="nav-number">3.</span> <span class="nav-text">绘制决策边界</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#logistic-rgeression-classifier-无法解决问题"><span class="nav-number">4.</span> <span class="nav-text">logistic rgeression classifier 无法解决问题</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#神经网络的初始化参数"><span class="nav-number">5.</span> <span class="nav-text">神经网络的初始化参数</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#计算损失函数"><span class="nav-number">6.</span> <span class="nav-text">计算损失函数</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#预测"><span class="nav-number">7.</span> <span class="nav-text">预测</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#模型训练"><span class="nav-number">8.</span> <span class="nav-text">模型训练</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#模型实例"><span class="nav-number">9.</span> <span class="nav-text">模型实例</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#不同的隐含层规模"><span class="nav-number">10.</span> <span class="nav-text">不同的隐含层规模</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#其他值得关注的点"><span class="nav-number">11.</span> <span class="nav-text">其他值得关注的点</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#查看tanh函数"><span class="nav-number">11.1.</span> <span class="nav-text">查看tanh函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#softmax函数"><span class="nav-number">11.2.</span> <span class="nav-text">softmax函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#关于隐含层的规模"><span class="nav-number">11.3.</span> <span class="nav-text">关于隐含层的规模</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#关于反向传播的细节"><span class="nav-number">11.4.</span> <span class="nav-text">关于反向传播的细节</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#变学习速率"><span class="nav-number">11.5.</span> <span class="nav-text">变学习速率</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#参考资料"><span class="nav-number">12.</span> <span class="nav-text">参考资料</span></a></li></ol></div>
            
          </div>
          <div class="post-toc-indicator-bottom post-toc-indicator"></div>
        </div>
      

    </div>
  </div>


    </div>

    <div id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">
  
  &copy; &nbsp;  2015 - 
  2016
  <span class="with-love">
    <i class="icon-heart"></i>
  </span>
  <span class="author">Ray</span>
</div>

<div class="powered-by">
  由 <a class="theme-link" href="http://hexo.io">Hexo</a> 強力驅動
</div>

<div class="theme-info">
  主題 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>



      </div>
    </div>

    <div class="back-to-top"></div>
  </div>

  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  

  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js"></script>
  <script type="text/javascript" src="/js/fancy-box.js?v=0.4.3"></script>


  <script type="text/javascript" src="/js/helpers.js?v=0.4.3"></script>
  

  <script type="text/javascript" src="/vendors/velocity/velocity.min.js"></script>
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js"></script>

  <script type="text/javascript" src="/js/motion_global.js?v=0.4.3" id="motion.global"></script>



  <script type="text/javascript" src="/js/search-toggle.js"></script>

  
  
<script type="text/javascript" src="/js/bootstrap.scrollspy.js?v=0.4.3" id="bootstrap.scrollspy.custom"></script>


<script type="text/javascript" id="sidebar.toc.highlight">
  $(document).ready(function () {
    var tocSelector = '.post-toc';
    var $tocSelector = $(tocSelector);
    var activeCurrentSelector = '.active-current';

    $tocSelector
      .on('activate.bs.scrollspy', function () {
        var $currentActiveElement = $(tocSelector + ' .active').last();

        removeCurrentActiveClass();
        $currentActiveElement.addClass('active-current');

        $tocSelector[0].scrollTop = $currentActiveElement.position().top;
      })
      .on('clear.bs.scrollspy', function () {
        removeCurrentActiveClass();
      });

    function removeCurrentActiveClass () {
      $(tocSelector + ' ' + activeCurrentSelector)
        .removeClass(activeCurrentSelector.substring(1));
    }

    function processTOC () {
      getTOCMaxHeight();
      toggleTOCOverflowIndicators();
    }

    function getTOCMaxHeight () {
      var $sidebarInner = $('.sidebar-inner');
      var height = $('.sidebar').height() -
                   $tocSelector.position().top -
                   $('.post-toc-indicator-bottom').height();

      $tocSelector.css('height', height);

      return height;
    }

    function toggleTOCOverflowIndicators () {
      tocOverflowIndicator(
        '.post-toc-indicator-top',
        $tocSelector.scrollTop() > 0 ? 'show' : 'hide'
      );

      tocOverflowIndicator(
        '.post-toc-indicator-bottom',
        $tocSelector.scrollTop() >= $tocSelector.find('ol').height() - $tocSelector.height() ? 'hide' : 'show'
      )
    }

    $(document).on('sidebar.didShow', function () {
      processTOC();
    });

    $('body').scrollspy({ target: tocSelector });
    $(window).on('resize', function () {
      if ( $('.sidebar').hasClass('sidebar-active') ) {
        processTOC();
      }
    });

    onScroll($tocSelector);

    function onScroll (element) {
      element.on('mousewheel DOMMouseScroll', function (event) {
          var oe = event.originalEvent;
          var delta = oe.wheelDelta || -oe.detail;
          var self = this;

          this.scrollTop += ( delta < 0 ? 1 : -1 ) * 30;
          event.preventDefault();

          toggleTOCOverflowIndicators();
      });
    }

    function tocOverflowIndicator (indicator, action) {
      $(indicator).velocity('stop').velocity({
        opacity: action === 'show' ? 0.4 : 0
      }, { duration: 100 });
    }

  });
</script>


  <script type="text/javascript" id="sidebar.nav">
    $(document).ready(function () {
      var html = $('html');

      $('.sidebar-nav li').on('click', function () {
        var item = $(this);
        var activeTabClassName = 'sidebar-nav-active';
        var activePanelClassName = 'sidebar-panel-active';
        if (item.hasClass(activeTabClassName)) {
          return;
        }

        var currentTarget = $('.' + activePanelClassName);
        var target = $('.' + item.data('target'));

        currentTarget.velocity('transition.slideUpOut', 200, function () {
          target
            .velocity('stop')
            .velocity('transition.slideDownIn', 200)
            .addClass(activePanelClassName);
        });

        item.siblings().removeClass(activeTabClassName);
        item.addClass(activeTabClassName);
      });

      $('.post-toc a').on('click', function (e) {
        e.preventDefault();
        var offset = $(escapeSelector(this.getAttribute('href'))).offset().top;
        html.velocity('stop').velocity('scroll', {
          offset: offset  + 'px',
          mobileHA: false
        });
      });

      // Expand sidebar on post detail page by default, when post has a toc.
      var $tocContent = $('.post-toc-content');
      if (isDesktop() && CONFIG.sidebar === 'post') {
        if ($tocContent.length > 0 && $tocContent.html().trim().length > 0) {
          displaySidebar();
        }
      }
    });
  </script>




  <script type="text/javascript">
    $(document).ready(function () {
      if (CONFIG.sidebar === 'always') {
        displaySidebar();
      }
    });
  </script>

  
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
  });
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


  
  

  
    
  

  <script type="text/javascript">
    var duoshuoQuery = {short_name:"blkstone"};
    (function() {
      var ds = document.createElement('script');
      ds.type = 'text/javascript';ds.async = true;
      ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
      ds.charset = 'UTF-8';
      (document.getElementsByTagName('head')[0]
      || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
  </script>

  


  
  

</body>
</html>
